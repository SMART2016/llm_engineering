# Decoding and Sampling Algorithms for Text Generation

## Introduction: How Do LLMs Choose Words?

When a language model like ChatGPT or GPT-2 generates text, it doesn't just "know" what to write next. Instead, for every single word position, the model produces a list of ALL possible words it could use, along with a score (called a "probability") for each word.

Think of it like this: Imagine you're writing a sentence that starts with "I love eating..." The model might give you scores like:
- "pizza" ‚Üí 30%
- "pasta" ‚Üí 25%
- "apples" ‚Üí 20%
- "books" ‚Üí 5%
- "cars" ‚Üí 3%
- ... and thousands of other words with smaller scores

**The big question is: How does the model pick which word to actually use?**

This document explains the different strategies (called "decoding methods") that models use to make this choice. Each method has trade-offs between being predictable vs. creative, fast vs. high-quality, and repetitive vs. diverse.

---

## How Auto-Regressive Generation Works

Before we dive into the methods, let's understand how text generation works step-by-step.

**Auto-regressive generation** is a fancy term that simply means: **the model generates one word at a time, using all the previous words as context**.

```mermaid
graph LR
    A["Start: I enjoy"] --> B["Model predicts: 'walking'"]
    B --> C["Context: I enjoy walking"]
    C --> D["Model predicts: 'with'"]
    D --> E["Context: I enjoy walking with"]
    E --> F["Model predicts: 'my'"]
    F --> G["And so on..."]
    
    style A fill:#e1f5ff
    style C fill:#e1f5ff
    style E fill:#e1f5ff
```

At each step:
1. The model looks at what's been written so far
2. It calculates probabilities for every possible next word
3. It uses a **decoding method** to pick one word
4. That word gets added to the context
5. Repeat until the model decides to stop (usually by generating a special "end" token)

Now let's explore the different ways to pick that next word!

---

## 1. Greedy Search: Always Pick the Most Likely Word

### The Simple Explanation

**Greedy Search** is the simplest approach: at every step, just pick the word with the highest probability. No thinking ahead, no randomness‚Äîjust always choose #1.

### How It Works

Imagine you're at each step and you have these options:

**Step 1:** Starting with "The"
- "nice" ‚Üí 50% ‚úÖ **Pick this!**
- "dog" ‚Üí 30%
- "big" ‚Üí 20%

**Step 2:** Now you have "The nice"
- "woman" ‚Üí 40% ‚úÖ **Pick this!**
- "person" ‚Üí 35%
- "man" ‚Üí 25%

**Step 3:** Now you have "The nice woman"
- "is" ‚Üí 60% ‚úÖ **Pick this!**
- "walks" ‚Üí 30%
- "said" ‚Üí 10%

```mermaid
graph TD
    Start["The"] --> A1["nice (50%)"]
    Start --> A2["dog (30%)"]
    Start --> A3["big (20%)"]
    
    A1 --> B1["woman (40%)"]
    A1 --> B2["person (35%)"]
    
    B1 --> C1["is (60%)"]
    B1 --> C2["walks (30%)"]
    
    style A1 fill:#90EE90
    style B1 fill:#90EE90
    style C1 fill:#90EE90
    style Start fill:#e1f5ff
```

### The Problem

While greedy search is fast and predictable, it has two major issues:

1. **It misses better paths**: Sometimes a slightly less likely word early on leads to a much better sentence later
2. **It creates repetition**: Models often get stuck in loops, repeating the same phrases over and over

### Real Example

> "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog. I'm not sure..."

Notice how it keeps repeating the same phrase? That's a classic problem with greedy search.

---

## 2. Beam Search: Keep Multiple Options Open

### The Simple Explanation

**Beam Search** is smarter than greedy search. Instead of only tracking the #1 choice, it keeps track of the top **N** candidates (called "beams") at each step. This way, it can explore multiple paths and pick the best one overall.

Think of it like exploring a maze: instead of always taking the first tunnel you see, you split into multiple groups to explore different paths simultaneously, then choose the path that gets you furthest.

### How It Works

Let's say we use **2 beams** (keeping track of 2 candidates):

**Step 1:** Starting with "The"
- **Beam 1:** "The nice" (50%)
- **Beam 2:** "The dog" (30%)

**Step 2:** For each beam, explore the next word:
- From "The nice": "The nice woman" (50% √ó 40% = 20%)
- From "The nice": "The nice person" (50% √ó 35% = 17.5%)
- From "The dog": "The dog has" (30% √ó 90% = 27%) ‚ú® **This is best!**
- From "The dog": "The dog is" (30% √ó 60% = 18%)

Now we keep:
- **Beam 1:** "The dog has" (27%)
- **Beam 2:** "The nice woman" (20%)

```mermaid
graph TD
    Start["The"] --> A1["nice (50%)"]
    Start --> A2["dog (30%)"]
    
    A1 --> B1["woman (20% total)"]
    A1 --> B2["person (17.5% total)"]
    
    A2 --> B3["has (27% total) ‚≠ê"]
    A2 --> B4["is (18% total)"]
    
    style A1 fill:#FFE4B5
    style A2 fill:#FFE4B5
    style B3 fill:#90EE90
    style B1 fill:#90EE90
    style Start fill:#e1f5ff
```

### The Benefit

By exploring multiple paths, beam search can find sequences that are better overall, even if they started with a less likely word.

### The Limitation

Beam search still tends to produce somewhat generic, "safe" text. It's great for tasks where you want the most likely, predictable output (like translation), but not ideal when you want creative or diverse text.

---

## 3. Sampling: Introducing Randomness

### The Simple Explanation

Instead of always picking the most likely word (greedy) or the most likely paths (beam search), **sampling** introduces **randomness**. The model picks words randomly, but **more likely words have a higher chance of being chosen**.

Think of it like a lottery where "pizza" gets 30 tickets, "pasta" gets 25 tickets, "apples" gets 20 tickets, etc. You randomly draw one ticket‚Äîso pizza is most likely to win, but pasta or apples could win too!

### How It Works

Given these probabilities:
- "walking" ‚Üí 40%
- "running" ‚Üí 30%
- "hiking" ‚Üí 20%
- "dancing" ‚Üí 10%

Instead of always picking "walking," we might pick:
- "walking" 40% of the time
- "running" 30% of the time
- "hiking" 20% of the time
- "dancing" 10% of the time

```mermaid
graph TD
    Start["I enjoy"] --> Random{Random Draw}
    Random -->|40% chance| A["walking"]
    Random -->|30% chance| B["running"]
    Random -->|20% chance| C["hiking"]
    Random -->|10% chance| D["dancing"]
    
    A --> Out1["...More diverse and creative outputs!"]
    B --> Out2["...Different each time!"]
    C --> Out3["...Sometimes surprising!"]
    D --> Out4["...Can be unexpected!"]
    
    style Start fill:#e1f5ff
    style Random fill:#FFD700
```

### Temperature: Controlling Creativity

**Temperature** is a setting that controls how "adventurous" the sampling is:

- **Low temperature (e.g., 0.5)**: Makes the model MORE predictable
  - "pizza" ‚Üí 50%, "pasta" ‚Üí 30%, "apples" ‚Üí 15%, "books" ‚Üí 5%
  - The gap between top choices becomes bigger
  
- **High temperature (e.g., 1.5)**: Makes the model MORE creative/random
  - "pizza" ‚Üí 25%, "pasta" ‚Üí 24%, "apples" ‚Üí 23%, "books" ‚Üí 20%
  - Probabilities become more evenly distributed

```mermaid
graph LR
    A["Low Temperature
    (Conservative)"] --> B["More Predictable
    Safer Choices"]
    
    C["High Temperature
    (Creative)"] --> D["More Random
    Surprising Choices"]
    
    style A fill:#ADD8E6
    style C fill:#FFB6C1
```

### The Problem with Pure Sampling

If we sample from ALL possible words (including very unlikely ones), the model might choose words that make no sense at all. Imagine if the lottery included a million tickets for nonsense words‚Äîyou'd sometimes get gibberish!

That's where the next two methods come in...

---

## 4. Top-K Sampling: Limiting the Lottery

### The Simple Explanation

**Top-K Sampling** solves the gibberish problem by saying: "Only consider the top **K** most likely words, ignore everything else."

For example, with K=5, you only put the top 5 words into the lottery, even though there might be thousands of possible words.

### How It Works

Let's say K=5 and you have these probabilities:

**All words:**
- "walking" ‚Üí 30%
- "running" ‚Üí 25%
- "hiking" ‚Üí 20%
- "cooking" ‚Üí 10%
- "reading" ‚Üí 8%
- "sleeping" ‚Üí 3%
- "jumping" ‚Üí 2%
- "flying" ‚Üí 1%
- ... thousands more tiny probabilities

**Top-K (K=5) only considers:**
- "walking" ‚Üí 30%
- "running" ‚Üí 25%
- "hiking" ‚Üí 20%
- "cooking" ‚Üí 10%
- "reading" ‚Üí 8%

Everything else is eliminated! Then we sample randomly from these 5 words (adjusting probabilities so they add up to 100%).

```mermaid
graph TD
    AllWords["All Possible Words
    (Thousands)"] --> Filter["Top-K Filter
    Keep only top 5"]
    
    Filter --> Top1["walking (30%)"]
    Filter --> Top2["running (25%)"]
    Filter --> Top3["hiking (20%)"]
    Filter --> Top4["cooking (10%)"]
    Filter --> Top5["reading (8%)"]
    
    Filter -.-> Rejected["sleeping, jumping,
    flying, etc. ‚ùå"]
    
    Top1 --> Sample["Random Sample
    from these 5"]
    Top2 --> Sample
    Top3 --> Sample
    Top4 --> Sample
    Top5 --> Sample
    
    style Filter fill:#FFD700
    style Rejected fill:#FFB6B6
```

### The Benefit

By eliminating low-probability words, Top-K prevents the model from choosing nonsensical words while still allowing for creativity and diversity.

### The Limitation

The problem with Top-K is that K is **fixed**. Sometimes you want to be MORE selective (when the model is very confident), and sometimes LESS selective (when the model is uncertain).

Example:
- If the next word is very obvious ("The capital of France is..."), you might only need K=2 
- If the next word is more open-ended ("I love to eat..."), you might want K=50

Top-K doesn't adapt to this‚Äîit always uses the same K value.

---

## 5. Top-P (Nucleus) Sampling: Adaptive Selection

### The Simple Explanation

**Top-P Sampling** (also called **Nucleus Sampling**) is smarter than Top-K. Instead of picking a fixed number of words, it picks **the smallest group of words whose probabilities add up to P%**.

For example, with P=90%, the model asks: "What's the smallest set of words that covers 90% of the probability?" Then it samples only from that set.

### How It Works

**Scenario 1:** Very confident prediction ("The capital of France is...")
- "Paris" ‚Üí 85%
- "paris" ‚Üí 7%
- "Lyon" ‚Üí 3%
- "France" ‚Üí 2%
- "French" ‚Üí 1%

With P=90%, we only need the first 2 words to reach 92% (85% + 7%). So we sample from just those 2!

**Scenario 2:** Uncertain prediction ("I love to eat...")
- "pizza" ‚Üí 15%
- "pasta" ‚Üí 12%
- "chicken" ‚Üí 11%
- "sushi" ‚Üí 10%
- "tacos" ‚Üí 9%
- "burgers" ‚Üí 8%
- ... (many more)

With P=90%, we need maybe 12 words to reach 90%. So we sample from 12 words!

```mermaid
graph TD
    subgraph Confident["Scenario 1: Confident"]
        C1["Paris (85%)"] --> CSum1["85%"]
        C2["paris (7%)"] --> CSum2["92% ‚úì Stop here!"]
        C3["Lyon (3%)"] -.-> Rejected1["Not needed ‚ùå"]
    end
    
    subgraph Uncertain["Scenario 2: Uncertain"]
        U1["pizza (15%)"] --> USum1["15%"]
        U2["pasta (12%)"] --> USum2["27%"]
        U3["chicken (11%)"] --> USum3["38%"]
        U4["...more words..."] --> USum4["...building up..."]
        U5["12th word"] --> USum5["92% ‚úì Stop here!"]
    end
    
    style CSum2 fill:#90EE90
    style USum5 fill:#90EE90
    style Rejected1 fill:#FFB6B6
```

### The Benefit

Top-P **adapts** to the situation:
- When the model is confident, it's more selective (picks from fewer words)
- When the model is uncertain, it's more open (picks from more words)

This creates more natural, human-like text compared to fixed Top-K.

### Real-World Results

With Top-P=0.92, models generate much more diverse and interesting text:

> "I enjoy walking with my cute dog for the rest of the day, but this had me staying in an unusual room and not going on nights out with friends..."

Notice how it's creative and varied, without being nonsensical or repetitive!

---

## Comparison: Which Method Should You Use?

Here's a simple guide:

```mermaid
graph TD
    Start["What kind of output
    do you need?"] --> Q1{Deterministic
    or Random?}
    
    Q1 -->|Deterministic
    Same output
    every time| Q2{Need to
    explore options?}
    
    Q1 -->|Random
    Different outputs
    each time| Q3{How much
    creativity?}
    
    Q2 -->|No, just
    fast results| Greedy["Greedy Search
    ‚ö° Fastest
    ‚ö†Ô∏è Can be repetitive"]
    
    Q2 -->|Yes, need
    best quality| Beam["Beam Search
    üìä Best for translation
    ‚úÖ More reliable"]
    
    Q3 -->|Moderate
    creativity| TopK["Top-K Sampling
    üé≤ Good balance
    üîß Fixed K value"]
    
    Q3 -->|High creativity
    + quality| TopP["Top-P Sampling
    ‚≠ê Most human-like
    üé® Adapts to context"]
    
    style Greedy fill:#FFB6B6
    style Beam fill:#ADD8E6
    style TopK fill:#DDA0DD
    style TopP fill:#90EE90
```

### Quick Reference Table

| Method | Randomness | Speed | Quality | Best For |
|--------|-----------|-------|---------|----------|
| **Greedy Search** | None | ‚ö°‚ö°‚ö° Fastest | ‚≠ê‚≠ê Can be repetitive | Quick tests, when speed matters |
| **Beam Search** | None | ‚ö°‚ö° Fast | ‚≠ê‚≠ê‚≠ê‚≠ê Reliable | Translation, summarization, factual tasks |
| **Sampling + Temperature** | High | ‚ö°‚ö° Fast | ‚≠ê‚≠ê‚≠ê Varies | When you want pure creativity |
| **Top-K Sampling** | Medium | ‚ö°‚ö° Fast | ‚≠ê‚≠ê‚≠ê‚≠ê Good | Creative writing, chat bots |
| **Top-P Sampling** | Medium | ‚ö°‚ö° Fast | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Best | Most general-purpose tasks, stories, dialogue |

---

## Practical Tips

### Combining Methods

You can combine these methods! For example:
- **Top-K + Top-P**: First filter to top 50 words (K=50), then do Top-P sampling within those
- **Beam Search + Sampling**: Use beam search but with some randomness
- **Temperature + Top-P**: Adjust temperature to control creativity, then use Top-P for adaptive selection

### Common Settings

Here are some typical settings for different use cases:

**Creative Writing:**
- Method: Top-P Sampling
- Temperature: 0.8-1.0
- Top-P: 0.9-0.95

**Chatbot Responses:**
- Method: Top-P Sampling
- Temperature: 0.7
- Top-P: 0.9

**Code Generation:**
- Method: Beam Search or Greedy
- Temperature: 0.2 (if using sampling)
- Beams: 3-5 (if using beam search)

**Translation:**
- Method: Beam Search
- Temperature: N/A
- Beams: 4-8

---

## Visualizing the Complete Process

Here's how it all comes together:

```mermaid
graph TD
    Input["Input Text:
    'I enjoy walking with my'"] --> Model["Language Model
    Processes Input"]
    
    Model --> Probs["Probability Distribution
    for Next Word:
    
    dog: 35%
    cat: 25%
    friend: 15%
    family: 12%
    ...thousands more..."]
    
    Probs --> Choose{Choose
    Decoding
    Method}
    
    Choose --> Method1["Greedy:
    Pick 'dog' (35%)"]
    
    Choose --> Method2["Beam:
    Track top 5 paths"]
    
    Choose --> Method3["Top-K:
    Sample from top 50"]
    
    Choose --> Method4["Top-P:
    Sample from words
    totaling 90%"]
    
    Method1 --> Output1["I enjoy walking with my dog"]
    Method2 --> Output2["I enjoy walking with my dog"]
    Method3 --> Output3["I enjoy walking with my cat"]
    Method4 --> Output4["I enjoy walking with my friend"]
    
    Output1 --> Repeat["Add chosen word
    to context and repeat
    for next word..."]
    Output2 --> Repeat
    Output3 --> Repeat
    Output4 --> Repeat
    
    style Input fill:#e1f5ff
    style Choose fill:#FFD700
    style Repeat fill:#DDA0DD
```

---

## Common Misconceptions

### "More randomness = better creativity" ‚ùå

**Not quite!** Too much randomness creates gibberish. The best creativity comes from:
- Medium temperature (0.7-1.0)
- Smart filtering (Top-P or Top-K)
- Letting the model use its training

### "Greedy search is always bad" ‚ùå

**Not true!** Greedy search is perfectly fine for:
- Quick prototypes
- When you need speed
- Deterministic outputs (same input always gives same output)

### "You always need the most advanced method" ‚ùå

**Nope!** Sometimes simpler is better:
- Greedy for speed
- Beam for reliability
- Sampling for creativity

Choose based on your specific needs!

---

## Key Takeaways

1. **Decoding methods determine how a language model picks the next word** from thousands of possibilities

2. **Greedy Search** is simple and fast but can be repetitive

3. **Beam Search** explores multiple paths to find better overall sequences

4. **Sampling** adds randomness for more creative and diverse outputs

5. **Temperature** controls how adventurous the sampling is

6. **Top-K Sampling** limits choices to the top K words to avoid gibberish

7. **Top-P (Nucleus) Sampling** is usually the best choice‚Äîit adapts to context and produces natural-sounding text

8. **Different tasks need different methods**‚Äîexperiment to find what works best for your use case!

---

## Further Reading

- [Hugging Face: How to Generate Text](https://huggingface.co/blog/how-to-generate) - The source material for this document
- [Hugging Face Generation Parameters](https://huggingface.co/docs/transformers/main_classes/text_generation) - Technical documentation
- Temperature and sampling in practice with different models
- The evolution of decoding methods in NLP research

---

**Remember:** There's no "perfect" decoding method‚Äîonly the method that's perfect for YOUR specific task! Experiment, compare outputs, and see what works best for your needs. üöÄ

